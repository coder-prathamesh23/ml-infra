# azure-pipelines-sagemaker-docker.yml
trigger: none

pool:
  name: dataengineering_tf        # your self-hosted agent pool

variables:
- group:        # variable group with your AWS creds

# Expected variables in tf-sagemaker-dev:
#   AWS_ACCOUNT_ID        = <12-digit ECR account ID>  (the 3... account)
#   AWS_REGION            = us-west-2
#   AWS_ACCESS_KEY_ID     = <access key for 3... account>
#   AWS_SECRET_ACCESS_KEY = <secret key for 3... account>
#   AWS_SESSION_TOKEN     = <optional, if using STS; otherwise empty>

stages:
- stage: RunSageMakerPipeline
  displayName: "Run SageMaker Pipeline via Docker (legacy AWS CLI)"
  jobs:
  - job: RunPipeline
    displayName: "Run pipeline inside ECR-based container"
    steps:

    # Step 0: Checkout repo (latest DS code)
    - checkout: self
      displayName: "Checkout repo (latest DS code)"

    # Step 1: Debug – prove which AWS account we are using
    - script: |
        echo "=== DEBUG: AWS identity from pipeline (should be ECR account) ==="
        echo "AWS_REGION      = $(AWS_REGION)"
        echo "AWS_ACCOUNT_ID  = $(AWS_ACCOUNT_ID)"

        echo
        echo "-> sts get-caller-identity"
        aws sts get-caller-identity --region $(AWS_REGION)
      displayName: "DEBUG – who am I (should be ECR account)"
      env:
        AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
        AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
        AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
        AWS_REGION: $(AWS_REGION)
        AWS_DEFAULT_REGION: $(AWS_REGION)

    # Step 2: (optional but useful) check that we can see the repo & images
    - script: |
        echo "=== DEBUG: ECR visibility from pipeline ==="

        echo "-> describe-repositories early-crew-dispatch-base"
        aws ecr describe-repositories \
          --repository-names early-crew-dispatch-base \
          --region $(AWS_REGION)

        echo
        echo "-> describe-images early-crew-dispatch-base"
        aws ecr describe-images \
          --repository-name early-crew-dispatch-base \
          --region $(AWS_REGION)
      displayName: "DEBUG – can I see the ECR repo & images?"
      env:
        AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
        AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
        AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
        AWS_REGION: $(AWS_REGION)
        AWS_DEFAULT_REGION: $(AWS_REGION)

    # Step 3: Login to ECR using legacy aws ecr get-login and pull the image
    - script: |
        set -e

        echo "Logging in to ECR using legacy aws ecr get-login..."

        # 1) Call the old CLI command (supported by your AWS CLI)
        LOGIN_CMD=$(aws ecr get-login --no-include-email --region $(AWS_REGION))

        echo "aws ecr get-login returned:"
        echo "$LOGIN_CMD"

        if [ -z "$LOGIN_CMD" ]; then
          echo "ERROR: aws ecr get-login returned empty output"
          exit 1
        fi

        # 2) Extract the password that appears after -p / --password
        PASSWORD=$(echo "$LOGIN_CMD" | awk '{for (i=1;i<=NF;i++) if ($i=="-p" || $i=="--password") {print $(i+1); exit}}')

        if [ -z "$PASSWORD" ]; then
          echo "ERROR: Could not extract password from aws ecr get-login output"
          exit 1
        fi

        # 3) Non-interactive Docker login using password-stdin
        echo "$PASSWORD" | docker login --username AWS --password-stdin \
          $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com

        echo "Pulling image with dependencies..."
        docker pull $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com/early-crew-dispatch-base:latest
      displayName: "Login to ECR and pull base image (legacy CLI)"
      env:
        AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
        AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
        AWS_SESSION_TOKEN: $(AWS_SESSION_TOKEN)
        AWS_REGION: $(AWS_REGION)
        AWS_DEFAULT_REGION: $(AWS_REGION)

    # Step 4: Run container, mount repo, execute run_pipeline.py inside it
    # Step 4: Run container, mount repo, execute run_pipeline.py inside it
    - script: |
        set -e

        echo "Running SageMaker pipeline inside container..."

        WORKSPACE="$(System.DefaultWorkingDirectory)"

        echo "Base identity before assume-role (should be IAM user):"
        aws sts get-caller-identity

        echo ""
        echo "Assuming SageMaker execution role: $SAGEMAKER_EXECUTION_ROLE_ARN"

        # Call STS to assume the SageMaker execution role
        CREDS=$(aws sts assume-role \
          --role-arn "$SAGEMAKER_EXECUTION_ROLE_ARN" \
          --role-session-name azdo-sagemaker-pipeline \
          --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \
          --output text)

        # CREDS is: "AKIA... SECRET... TOKEN..."
        read ROLE_AK ROLE_SK ROLE_TOKEN <<< "$CREDS"

        export ROLE_AWS_ACCESS_KEY_ID="$ROLE_AK"
        export ROLE_AWS_SECRET_ACCESS_KEY="$ROLE_SK"
        export ROLE_AWS_SESSION_TOKEN="$ROLE_TOKEN"

        echo ""
        echo "Identity after assume-role (should be assumed-role/SageMaker-ExecutionRole-Admin/...):"
        AWS_ACCESS_KEY_ID=$ROLE_AWS_ACCESS_KEY_ID \
        AWS_SECRET_ACCESS_KEY=$ROLE_AWS_SECRET_ACCESS_KEY \
        AWS_SESSION_TOKEN=$ROLE_AWS_SESSION_TOKEN \
          aws sts get-caller-identity

        echo ""
        echo "Starting Docker container with assumed-role credentials..."

        docker run --rm \
          -e AWS_ACCESS_KEY_ID=$ROLE_AWS_ACCESS_KEY_ID \
          -e AWS_SECRET_ACCESS_KEY=$ROLE_AWS_SECRET_ACCESS_KEY \
          -e AWS_SESSION_TOKEN=$ROLE_AWS_SESSION_TOKEN \
          -e AWS_REGION=$(AWS_REGION) \
          -e AWS_DEFAULT_REGION=$(AWS_REGION) \
          -e OPENBLAS_NUM_THREADS=1 \
          -e OMP_NUM_THREADS=1 \
          -e MKL_NUM_THREADS=1 \
          -e NUMEXPR_NUM_THREADS=1 \
          -e PYTHONPATH=/workspace/early_crew_dispatch \
          -v "$WORKSPACE":/workspace \
          -w /workspace/early_crew_dispatch/ml_pipeline \
          $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com/early-crew-dispatch-base:latest \
          python ../run_pipeline.py
      displayName: "Mount repo & run run_pipeline.py in container"
      env:
        # IAM user credentials – used only to call sts:AssumeRole
        AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
        AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
        AWS_REGION: $(AWS_REGION)
        AWS_DEFAULT_REGION: $(AWS_REGION)
        # Execution role to assume
        SAGEMAKER_EXECUTION_ROLE_ARN: $(SAGEMAKER_EXECUTION_ROLE_ARN)


import sys
import boto3
from urllib.parse import urlparse

from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job


# -----------------------------
# Helpers
# -----------------------------
def s3_parse(s3_uri: str):
    u = urlparse(s3_uri)
    if u.scheme != "s3":
        raise ValueError(f"Expected s3:// URI, got: {s3_uri}")
    bucket = u.netloc
    key = u.path.lstrip("/")
    return bucket, key


def s3_list_keys(bucket: str, prefix: str):
    s3 = boto3.client("s3")
    keys = []
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            keys.append(obj["Key"])
    return keys


def s3_has_parquet(bucket: str, prefix: str) -> bool:
    s3 = boto3.client("s3")
    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=50)
    for obj in resp.get("Contents", []):
        if obj["Key"].endswith(".parquet"):
            return True
    return False


def s3_delete_prefix(bucket: str, prefix: str):
    s3 = boto3.client("s3")
    keys = s3_list_keys(bucket, prefix)
    if not keys:
        return
    for i in range(0, len(keys), 1000):
        chunk = keys[i:i + 1000]
        s3.delete_objects(
            Bucket=bucket,
            Delete={"Objects": [{"Key": k} for k in chunk]}
        )


def get_arg(argv, name, default=None):
    flag = f"--{name}"
    if flag in argv:
        i = argv.index(flag)
        if i + 1 < len(argv):
            return argv[i + 1]
    return default


# -----------------------------
# Required args
# -----------------------------
# Required:
# --JOB_NAME
# --src_base (s3://bucket/prefix/)
# --dst_base (s3://bucket/prefix/)
# --year --month --day
req = ["JOB_NAME", "src_base", "dst_base", "year", "month", "day"]
args = getResolvedOptions(sys.argv, req)

# Optional:
# --hour (e.g. 2)
# --output_files (default 8; set 1 if you truly want single file output)
hour = get_arg(sys.argv, "hour", None)
output_files = int(get_arg(sys.argv, "output_files", "8"))

src_base = args["src_base"].rstrip("/") + "/"
dst_base = args["dst_base"].rstrip("/") + "/"

# Numeric partition layout: YYYY/M/D/(H/)
y = str(int(args["year"]))
m = str(int(args["month"]))
d = str(int(args["day"]))

partition_path = f"{y}/{m}/{d}/"
if hour is not None:
    partition_path += f"{int(hour)}/"  # e.g. "2/"

src_path = src_base + partition_path
dst_final = dst_base + partition_path
dst_tmp = dst_base + "_tmp/" + partition_path  # staging publish path

print("========== COMPACTION INPUTS ==========")
print(f"src_path     = {src_path}")
print(f"dst_tmp      = {dst_tmp}")
print(f"dst_final    = {dst_final}")
print(f"output_files = {output_files}")
print("=======================================")

# Validate input exists (parquet files)
src_bucket, src_prefix = s3_parse(src_path)
if not s3_has_parquet(src_bucket, src_prefix):
    print(f"No parquet found under {src_path}. Exiting without changes.")
    sys.exit(0)

# -----------------------------
# Spark/Glue init
# -----------------------------
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

job = Job(glueContext)
job.init(args["JOB_NAME"], args)

# Reasonable defaults
spark.conf.set("spark.sql.parquet.mergeSchema", "false")
spark.conf.set("spark.sql.files.ignoreCorruptFiles", "true")
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

# -----------------------------
# Read raw parquet
# -----------------------------
df = spark.read.parquet(src_path)

# -----------------------------
# Reduce output file count
# -----------------------------
# coalesce reduces partitions without full shuffle; good for compaction output
current_parts = df.rdd.getNumPartitions()
if output_files <= 0:
    raise ValueError("output_files must be >= 1")

if output_files < current_parts:
    df_out = df.coalesce(output_files)
elif output_files > current_parts:
    df_out = df.repartition(output_files)
else:
    df_out = df

# -----------------------------
# Write to TEMP
# -----------------------------
# Overwrite temp partition path so reruns are clean
df_out.write.mode("overwrite").parquet(dst_tmp)

# -----------------------------
# Publish (TEMP -> FINAL)
# -----------------------------
dst_bucket, dst_prefix_final = s3_parse(dst_final)
tmp_bucket, tmp_prefix = s3_parse(dst_tmp)

s3 = boto3.client("s3")

# Delete existing FINAL partition
s3_delete_prefix(dst_bucket, dst_prefix_final)

# Copy objects from TEMP -> FINAL
tmp_keys = s3_list_keys(tmp_bucket, tmp_prefix)
for k in tmp_keys:
    rel = k[len(tmp_prefix):]  # preserve relative structure
    target_key = dst_prefix_final + rel
    s3.copy_object(
        Bucket=dst_bucket,
        Key=target_key,
        CopySource={"Bucket": tmp_bucket, "Key": k}
    )

# Cleanup TEMP
s3_delete_prefix(tmp_bucket, tmp_prefix)

print("Compaction complete and published to final destination.")
job.commit()


flowchart LR
  %% =======================
  %% HUB (shared services)
  %% =======================
  subgraph HUB["Connectivity / Hub Subscription"]
    vwan["vWAN Hub (Connectivity)"]
    dns["Private DNS Zones + DNS Resolver\n(shared / central)"]
  end

  %% =======================
  %% DATA PLATFORM SPOKE
  %% =======================
  subgraph DP["Data Platform Subscription (Spoke)"]
    dpvnet["Landing Zone VNet\n(min subnets + PE subnet if needed)"]
    dpcore["Core RG:\nKey Vault + Monitoring\n(+ optional Storage)"]
    cap["Fabric Capacity (Azure resource)"]
  end

  fabric["Microsoft Fabric (SaaS)\nWorkspaces / OneLake"]

  %% =======================
  %% DATA SCIENCE SPOKE
  %% =======================
  subgraph DS["Data Science Subscription (Spoke)"]
    dsvnet["Landing Zone VNet\n(workload subnet + PE subnet)"]
    pesub["Private Endpoint Subnet"]
    dscore["Core RG:\nKey Vault + Storage + ACR\n+ Monitoring"]
    amlws["Azure ML Workspace (control plane)"]

    subgraph AML["Azure ML - Managed Network Isolation"]
      mnet["Microsoft Managed VNet"]
      compute["Training + Batch/Online Compute"]
    end
  end

  %% =======================
  %% HUB-AND-SPOKE LINKS (what they pointed at)
  %% =======================
  vwan ---|"vHub connection / VNet peering"| dpvnet
  vwan ---|"vHub connection / VNet peering"| dsvnet

  dns -. "Private DNS VNet links" .-> dpvnet
  dns -. "Private DNS VNet links" .-> dsvnet

  %% =======================
  %% DS internal wiring
  %% =======================
  dsvnet --> pesub
  dscore --- amlws
  pesub -->|"Private Endpoint(s)"| amlws
  mnet --> compute
  compute -->|"Private access via PE + DNS"| dscore

  %% =======================
  %% Fabric relationship
  %% =======================
  cap --> fabric
  compute -. "HTTPS to OneLake/Fabric\n(Controlled by Entra + Fabric CA)" .-> fabric

  note["Open item (BIGGEST RISK):\nHow will Fabric Conditional Access treat traffic from AML Managed VNet?\nOption A: CA exception to allow AML Managed VNet.\nOption B (fallback): move to Customer VNet injection\n(and route via hub controls/firewall)."]
  note --- fabric
  note --- mnet
